import os
import sys
import re
import json
import time
import statistics
import pymysql
import random
from flask import Flask, render_template, request, redirect, url_for, session, flash
from werkzeug.security import generate_password_hash, check_password_hash
from werkzeug.utils import secure_filename
from datetime import datetime
from dotenv import load_dotenv
from kiwipiepy import Kiwi
from soynlp.normalizer import repeat_normalize
# -----------------------
# [0] í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
# -----------------------
load_dotenv()
app = Flask(__name__)
app.secret_key = 'echomind_secret_key_secure_random_string'

# Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
kiwi = Kiwi()

# -----------------------
# [DB ì„¤ì •]
# -----------------------
print("\n" + "="*40)
print("   EchoMind DB ì ‘ì† ì„¤ì •")
print("="*40)

db_config = {
    'host': 'echomind-db.cbqkoi8kaesl.ap-northeast-2.rds.amazonaws.com',
    'user': 'admin',        # ì‚¬ìš©ìë‹˜ì´ ì„¤ì •í•œ ID
    'password': 'mypassword1234',    # ì‚¬ìš©ìë‹˜ì´ ì„¤ì •í•œ ë¹„ë²ˆ
    'db': 'echomind',
    'charset': 'utf8mb4',
    'cursorclass': pymysql.cursors.DictCursor
}

UPLOAD_FOLDER = 'uploads'
ALLOWED_EXTENSIONS = {'txt', 'csv'}
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER

if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

# -----------------------
# [1] KNU ê°ì„± ì‚¬ì „ ë¡œë”© (ë¡œì»¬ íŒŒì¼)
# -----------------------
print(">>> KNU ê°ì„± ì‚¬ì „ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...")
SENTIMENT_DB = {}
try:
    with open('SentiWord_info.json', encoding='utf-8-sig') as f:
        senti_data = json.load(f)
    
    for entry in senti_data:
        root_word = entry['word_root']
        score = int(entry['polarity']) # -2 ~ 2
        SENTIMENT_DB[root_word] = score
        SENTIMENT_DB[entry['word']] = score 

    print(f">>> ì‚¬ì „ ë¡œë”© ì™„ë£Œ! (ë‹¨ì–´ ìˆ˜: {len(SENTIMENT_DB)}ê°œ)")
except Exception as e:
    print(f">>> [ê²½ê³ ] 'SentiWord_info.json' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤! ê°ì„± ë¶„ì„ì´ ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print(f">>> ì—ëŸ¬ ë‚´ìš©: {e}")

# -----------------------
# [2] ë¶„ì„ ë¡œì§ ëª¨ìŒ
# -----------------------
# -----------------------
# [2] ë¶„ì„ ë¡œì§ ëª¨ìŒ (MBTI Linguistic Features)
# -----------------------
LINE_RE = re.compile(r"^\[(?P<name>.+?)\] \[(?P<time>.+?)\] (?P<msg>.*)")
ANDROID_RE = re.compile(r"^(?P<time>\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼ (ì˜¤ì „|ì˜¤í›„) \d{1,2}:\d{2}), (?P<name>.+?) : (?P<msg>.*)")
# -----------------------
# [1.1] ìš•ì„¤/ë…ì„± ì‚¬ì „ ë¡œë”© (korean_bad_words.json)
# -----------------------
print(">>> ë…ì„±(ìš•ì„¤) ì‚¬ì „ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...")
BAD_WORDS = set()
try:
    with open('korean_bad_words.json', encoding='utf-8') as f:
        bad_data = json.load(f)
        # bad_dataê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€, ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸ í•„ìš” (ë³´í†µ ë¦¬ìŠ¤íŠ¸ ['ì‹œë°œ', ...])
        # ë§Œì•½ {"bad_words": [...]} í˜•íƒœë©´ keys í™•ì¸
        if isinstance(bad_data, list):
            # Check if elements are dicts or strings
            if bad_data and isinstance(bad_data[0], dict):
                # Structure: [{"text": "word", ...}, ...]
                BAD_WORDS = {item.get('text') for item in bad_data if item.get('text')}
            else:
                # Structure: ["word", ...]
                BAD_WORDS = set(bad_data)
        elif isinstance(bad_data, dict) and "bad_words" in bad_data:
            BAD_WORDS = set(bad_data["bad_words"])
        else:
            # ë”•ì…”ë„ˆë¦¬ í‚¤ ìì²´ê°€ ë‹¨ì–´ì¼ ìˆ˜ë„ ìˆìŒ
            BAD_WORDS = set(bad_data.keys())
            
    # í•µì‹¬ ìš•ì„¤ì€ ê°•ì œ ì¶”ê°€ (ëˆ„ë½ ë°©ì§€)
    BAD_WORDS.update(["ì‹œë°œ", "ì”¨ë°œ", "ë³‘ì‹ ", "ê°œìƒˆë¼", "ì¡´ë‚˜", "ë¯¸ì¹œ", "ì£½ì–´", "êº¼ì ¸", "ë…„", "ë†ˆ"])
    print(f">>> ë…ì„± ì‚¬ì „ ë¡œë”© ì™„ë£Œ! (ë‹¨ì–´ ìˆ˜: {len(BAD_WORDS)}ê°œ)")
except Exception as e:
    print(f">>> [ê²½ê³ ] 'korean_bad_words.json' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤! ê¸°ë³¸ ìš•ì„¤ ëª©ë¡ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    BAD_WORDS = {"ì‹œë°œ", "ì”¨ë°œ", "ë³‘ì‹ ", "ê°œìƒˆë¼", "ì¡´ë‚˜", "ë¯¸ì¹œ", "ì£½ì–´", "êº¼ì ¸", "ë…„", "ë†ˆ"}


# =============================================================================
# [ì‚¬ìš©ì ì„¤ì •] MBTI ê°€ì¤‘ì¹˜ ì¡°ì ˆ
# ... (ê¸°ì¡´ ì„¤ì • ìœ ì§€) ...
WEIGHT_E = 1.0; WEIGHT_I = 1.2
WEIGHT_S = 1.0; WEIGHT_N = 1.0
WEIGHT_T = 1.0; WEIGHT_F = 1.0
WEIGHT_J = 1.0; WEIGHT_P = 1.2
# =============================================================================

# ... (ê¸°ì¡´ í‚¤ì›Œë“œ ìœ ì§€) ...
E_ENDINGS = {'ì–´', 'ì', 'ê¹Œ', 'í•´', 'ë´', 'ì§€', 'ë‹ˆ', 'ëƒ'}
I_ENDINGS = {'ë‹¤', 'ìŒ', 'ì„', 'ì…ˆ', 'í•¨', 'ë“¯', 'ë„¤'}
# ...

# [ì¶”ê°€] SKIP_TOKENS ì •ì˜ (ëˆ„ë½ ìˆ˜ì •)
SKIP_TOKENS = {'ì‚¬ì§„', 'ë™ì˜ìƒ', 'ì´ëª¨í‹°ì½˜', 'ë³´ì´ìŠ¤í†¡ í•´ìš”.', 'í˜ì´ìŠ¤í†¡ í•´ìš”.', 'íŒŒì¼'}

# -----------------------------------------------------------------------------
# [ì¶”ê°€] Big5 ê³ ë„í™”ë¥¼ ìœ„í•œ í•œêµ­ì–´ ìŠ¤íƒ€ì¼ ë¶„ì„ (API ë¡œì§ ì´ì‹)
# -----------------------------------------------------------------------------
def analyze_korean_style_features(sentences):
    """
    Big5 ì‚°ì¶œì„ ìœ„í•œ ì •ë°€ ì–¸ì–´ ìŠ¤íƒ€ì¼ ë¶„ì„
    - TTR (ì–´íœ˜ ë‹¤ì–‘ì„± -> ê°œë°©ì„±)
    - Self-Ref (ìê¸° ì§€ì¹­ -> ì™¸í–¥ì„±)
    - Laughs (ë¦¬ì•¡ì…˜ -> ì™¸í–¥ì„±/ì¹œí™”ì„±)
    - Certainty (í™•ì‹ ì–´ -> ì„±ì‹¤ì„±/J)
    """
    full_text = " ".join(sentences[:5000])
    tokens = kiwi.tokenize(full_text)
    total_words = len(tokens)
    if total_words == 0: total_words = 1
    
    unique_morphs = set()
    self_ref_count = 0
    certainty_count = 0
    uncertainty_count = 0
    laugh_count = 0 
    
    # ìê¸°ì§€ì‹œì–´ (ë‚˜, ì €, ìš°ë¦¬...)
    self_pronouns = {"ë‚˜", "ì €", "ìš°ë¦¬", "ë‚´", "ì œ"}
    # í™•ì‹ ì–´
    certainty_words = {"ì§„ì§œ", "ì •ë§", "ë„ˆë¬´", "ì™„ì „", "í™•ì‹¤íˆ", "ë¶„ëª…", "ë°˜ë“œì‹œ", "ë¬¼ë¡ ", "ì ˆëŒ€", "ë‹¹ì—°"}
    # ë¶ˆí™•ì‹ ì–´
    uncertainty_words = {"ì•„ë§ˆ", "ê¸€ì„", "ì•½ê°„", "ì¢€", "ì–´ì©Œë©´", "ëª¨ë¥´", "ë“¯"}
    
    for t in tokens:
        m, tag = t.form, t.tag
        unique_morphs.add(m)
        
        # 1. ìê¸°ì§€ì¹­ (NP: ëŒ€ëª…ì‚¬, MM: ê´€í˜•ì‚¬)
        if m in self_pronouns and (tag.startswith('N') or tag == 'MM'):
            self_ref_count += 1
            
        # 2. ë¦¬ì•¡ì…˜ (ã…‹, ã…, ã… ) - ì›ƒìŒì†Œë¦¬
        if any(c in m for c in ['ã…‹', 'ã…', 'ã… ', 'ã…œ']):
            laugh_count += 1
            
        # 3. í™•ì‹ /ë¶ˆí™•ì‹ 
        if m in certainty_words: certainty_count += 1
        if m in uncertainty_words: uncertainty_count += 1
            
    # Normalize (ë¹ˆë„ ë¹„ìœ¨)
    style = {
        "avg_len": sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0, # ì–´ì ˆ ê¸°ì¤€ í‰ê·  ê¸¸ì´
        "ttr": len(unique_morphs) / total_words,               # ì–´íœ˜ ë‹¤ì–‘ì„±
        "self_ref": self_ref_count / total_words,              # ìê¸° ì¤‘ì‹¬ì„±
        "laughs": laugh_count / total_words,                   # ë¦¬ì•¡ì…˜ ë¹„ìœ¨
        "certainty": certainty_count / total_words,            # í™•ì‹  ë¹„ìœ¨
        "uncertainty": uncertainty_count / total_words         # ë¶ˆí™•ì‹  ë¹„ìœ¨
    }
    return style

def calculate_advanced_big5(style, tox_ratio, pos_ratio, neg_ratio):
    """
    ìŠ¤íƒ€ì¼(Style) + ê°ì„±(Sentiment) + ë…ì„±(Toxicity) -> Big5 ì ìˆ˜ ì‚°ì¶œ
    [v3] ì •ê·œí™” ê°€ì¤‘í•© ë°©ì‹ - 0~100 ì „ ë²”ìœ„ ì‚¬ìš© ê°€ëŠ¥
    """
    
    def normalize(val, scale=1.0, offset=0.0):
        """ê°’ì„ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™” (ìŠ¤ì¼€ì¼ ì¦í­ + ì˜¤í”„ì…‹ ì§€ì›)"""
        val = (val * scale) + offset
        return min(1.0, max(0.0, val))

    # ==========================================================================
    # 1. ê°œë°©ì„± (Openness) - ì–´íœ˜ ë‹¤ì–‘ì„± + ë¬¸ì¥ ê¸¸ì´(ë‹¨ë‹µí˜• X)
    # TTRì´ ë†’ê³ , ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ì§€ ì•Šìœ¼ë©´ ê°œë°©ì 
    # ==========================================================================
    openness = (0.6 * normalize(style['ttr'], 2.5)) + \
               (0.2 * normalize(style['avg_len'] / 15)) + \
               (0.2 * normalize(pos_ratio, 1.2))

    # ==========================================================================
    # 2. ì„±ì‹¤ì„± (Conscientiousness) - í™•ì‹ ì–´ + ë¦¬ì•¡ì…˜ ì ˆì œ + ì˜ˆì˜(ë…ì„±â†“)
    # í™•ì‹ ì–´ ë§ê³ , ã…‹ã…‹ã…‹ ì ë‹¹í•˜ê³ , ìš•ì„¤ ì—†ìœ¼ë©´ ì„±ì‹¤
    # ==========================================================================
    conscientiousness = (0.4 * normalize(style['certainty'], 15.0)) + \
                        (0.3 * (1 - normalize(style['laughs'], 5.0))) + \
                        (0.3 * (1 - tox_ratio))

    # ==========================================================================
    # 3. ì™¸í–¥ì„± (Extraversion) - ë¦¬ì•¡ì…˜ + ê¸ì • + ìê¸°í‘œí˜„
    # ã…‹ã…‹ã…‹ ë§ê³ , ê¸ì •ì ì´ê³ , ìê¸° ì´ì•¼ê¸° ë§ì´ í•˜ë©´ ì™¸í–¥
    # ==========================================================================
    extraversion = (0.4 * normalize(style['laughs'], 10.0)) + \
                   (0.3 * normalize(pos_ratio, 1.5)) + \
                   (0.3 * normalize(style['self_ref'], 20.0))

    # ==========================================================================
    # 4. ì¹œí™”ì„± (Agreeableness) - ë…ì„±â†“ + ë¦¬ì•¡ì…˜ + ê¸ì •
    # ìš•ì„¤ ì—†ê³ , ë¦¬ì•¡ì…˜ ì¢‹ê³ , ê¸ì •ì ì´ë©´ ì¹œí™”ì 
    # ==========================================================================
    neutral_ratio = 1 - pos_ratio - neg_ratio  # ì¤‘ë¦½ ë¹„ìœ¨ ì¶”ì •
    agreeableness = (0.35 * (1 - tox_ratio)) + \
                    (0.25 * normalize(style['laughs'], 8.0)) + \
                    (0.2 * normalize(pos_ratio, 1.2)) + \
                    (0.2 * max(0, neutral_ratio))

    # ==========================================================================
    # 5. ì‹ ê²½ì„± (Neuroticism) - ë¶€ì • + ë¶ˆí™•ì‹  + ë…ì„±
    # ë¶€ì •ì ì´ê³ , ë¶ˆí™•ì‹¤í•˜ê³ , ê³µê²©ì ì´ë©´ ì‹ ê²½ì¦ ë†’ìŒ
    # ** ëª¨ë‘ 0ì´ë©´ 0ì , ëª¨ë‘ ë†’ìœ¼ë©´ 100ì  **
    # ==========================================================================
    neuroticism = (0.5 * neg_ratio) + \
                  (0.3 * normalize(style['uncertainty'], 8.0)) + \
                  (0.2 * tox_ratio)

    # ==========================================================================
    # ìµœì¢…: 0~1 ê°’ì„ 0~100ìœ¼ë¡œ ë³€í™˜ (ë°˜ì˜¬ë¦¼)
    # ==========================================================================
    return {
        "openness": round(openness * 100, 1),
        "conscientiousness": round(conscientiousness * 100, 1),
        "extraversion": round(extraversion * 100, 1),
        "agreeableness": round(agreeableness * 100, 1),
        "neuroticism": round(neuroticism * 100, 1),
    }

# 2. S vs N
# [ìˆ˜ì •] S í‚¤ì›Œë“œì—ì„œ ë„ˆë¬´ í”í•œ ì‹œì  ë‹¨ì–´ ì œê±° ('ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ì§€ê¸ˆ' ë“±ì€ ëˆ„êµ¬ë‚˜ ì”€)
S_KEYWORDS = {'í˜„ì‹¤', 'ì‚¬ì‹¤', 'íŒ©íŠ¸', 'ë°ì´í„°', 'ê²€ì¦', 'ì¦ëª…', 'í™•ì¸', 'ê²½í—˜', 'ê´€ì°°', 'ì‹¤ì œ', 'êµ¬ì²´', 'ë””í…Œì¼', 'ì„¸ë¶€', 'ì˜¤ê°', 'ì²´í—˜', 'ì‹¤ìš©', 'ë„êµ¬', 'ì¬ë£Œ', 'ìˆ˜ì¹˜', 'í†µê³„', 'ì¦ê±°', 'ê¸°ë¡', 'ì§€ê¸ˆ', 'ì˜¤ëŠ˜', 'ì—¬ê¸°'} # [ìˆ˜ì •] ë³µêµ¬ëœ í‘œí˜„ í‚¤ì›Œë“œ
N_KEYWORDS = {'ìƒìƒ', 'ë§Œì•½', 'í˜¹ì‹œ', 'ê°€ëŠ¥ì„±', 'ë¯¸ë˜', 'ì˜ë¯¸', 'ë¹„ì „', 'ì•„ì´ë””ì–´', 'ì§ê´€', 'ì˜ê°', 'ì² í•™', 'ìš°ì£¼', 'ê°€ì¹˜ê´€', 'ì»¨ì…‰', 'ë¹„ìœ ', 'ì€ìœ ', 'ìƒì§•', 'ì¶”ìƒ', 'ë§ìƒ', 'ê¿ˆ', 'ì´ìƒ', 'ì›ë¦¬', 'íŒ¨í„´', 'ì•„ë§ˆ'} # [ìˆ˜ì •] ë³µêµ¬ëœ í‘œí˜„ í‚¤ì›Œë“œ

# 3. T vs F
T_KEYWORDS = {'ë¶„ì„', 'ë…¼ë¦¬', 'ì›ì¸', 'ê²°ê³¼', 'ì´ìœ ', 'ê·¼ê±°', 'íš¨ìœ¨', 'ë¹„íŒ', 'í‰ê°€', 'íŒë‹¨', 'ê°ê´€', 'ì›ì¹™', 'ì •ì˜', 'ê²€í† ', 'í•´ê²°', 'ë°©ë²•', 'ì‹œìŠ¤í…œ', 'ê¸°ëŠ¥', 'ì„±ëŠ¥', 'ì„¤ëª…', 'ì´í•´', 'ì¸ê³¼', 'ì™œ', 'ë•Œë¬¸ì—', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ì¦‰'} # [ìˆ˜ì •] ë³µêµ¬ëœ í‘œí˜„ í‚¤ì›Œë“œ
F_KEYWORDS = {'ê³µê°', 'ê°ë™', 'ì„œìš´', 'í–‰ë³µ', 'ê°ì‚¬', 'ë¯¸ì•ˆ', 'ê³ ë§ˆì›Œ', 'ì†Œì¤‘', 'ë°°ë ¤', 'ì‘ì›', 'ìœ„ë¡œ', 'ì¢‹ì•„', 'ì‹«ì–´', 'ë§ˆìŒ', 'ê¸°ë¶„', 'ëŠë‚Œ', 'ì„¼ìŠ¤', 'ì¡°í™”', 'ê´€ê³„', 'ì‚¬ëŒ', 'ì‚¬ë‘', 'ìš°ì •', 'ê°ì •', 'í™”ì´íŒ…', 'ì§„ì§œ', 'ë„ˆë¬´', 'ì™„ì „', 'ëŒ€ë°•', 'í—', 'ã… ã… ', 'ã…œã…œ'} # [ìˆ˜ì •] ë³µêµ¬ëœ í‘œí˜„ í‚¤ì›Œë“œ

# 4. J vs P
J_KEYWORDS = {'ê³„íš', 'ì¼ì •', 'ì¤€ë¹„', 'ì •ë¦¬', 'ì²´ê³„', 'ìˆœì„œ', 'ë‹¨ê³„', 'ëª©í‘œ', 'ë§ˆê°', 'ê¸°í•œ', 'ì™„ë£Œ', 'ë‹¬ì„±', 'ì•½ì†', 'ê·œì¹™', 'í†µì œ', 'ë¯¸ë¦¬', 'ì˜ˆì•½', 'ìŠ¤ì¼€ì¤„', 'ë¦¬ìŠ¤íŠ¸', 'ì ˆì°¨', 'í™•ì •'} # [ìˆ˜ì •] ë³µêµ¬ëœ í‘œí˜„ í‚¤ì›Œë“œ
P_KEYWORDS = {'ìƒí™©', 'ë³€ë™', 'ìœ ë™', 'ê·¸ë•Œ', 'ë´ì„œ', 'ì¦‰í¥', 'ììœ ', 'ì¬ë¯¸', 'ê³¼ì •', 'íƒìƒ‰', 'ê²½í—˜', 'ì˜¤í”ˆ', 'ê°€ëŠ¥', 'ìœµí†µ', 'ì ì‘', 'ë³€í™”', 'ì–´ë–»ê²Œë“ ', 'ë†€ì', 'ì—¬ìœ ', 'ëª°ë¼', 'ì•„ë¬´ê±°ë‚˜', 'ê·¸ëƒ¥', 'ì¼ë‹¨'} # [ìˆ˜ì •] ë³µêµ¬ëœ í‘œí˜„ í‚¤ì›Œë“œ

def parse_kakao_txt(path: str):
    rows = []
    try:
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            for line in f:
                line = line.rstrip("\n")
                m = LINE_RE.match(line)
                if m:
                    rows.append({
                        "speaker": m.group("name").strip(),
                        "time": m.group("time"),
                        "text": m.group("msg").strip(),
                    })
                    continue

                m2 = ANDROID_RE.match(line)
                if m2:
                    rows.append({
                        "speaker": m2.group("name").strip(),
                        "time": m2.group("time"),
                        "text": m2.group("msg").strip(),
                    })
                    continue
    except Exception as e:
        print(f"File parsing error: {e}")
    return rows

def clean_text(t: str) -> str:
    t = re.sub(r"https?://\S+", " ", t)
    t = re.sub(r"ì´ëª¨í‹°ì½˜|ì‚¬ì§„|ë™ì˜ìƒ", "", t) 
    t = re.sub(r"[^ê°€-í£a-zA-Z0-9\s\.\?!]", " ", t) # íŠ¹ìˆ˜ë¬¸ì ì¼ë¶€ í—ˆìš©
    t = re.sub(r"\s+", " ", t).strip()
    return t

# [ì‹œê°„ íŒŒì‹± í—¬í¼ - ê°œì„ ëœ ë²„ì „]
def parse_time_diff(t1_str, t2_str):
    """
    ë‹¤ì–‘í•œ í¬ë§·ì˜ ì‹œê°„ ë¬¸ìì—´ ì°¨ì´ë¥¼ ë¶„ ë‹¨ìœ„ë¡œ ê³„ì‚°
    ì§€ì› í¬ë§·: "ì˜¤ì „ 10:23", "ì˜¤í›„ 2:05", "AM 10:23", "PM 2:05", "14:30"
    """
    def time_to_min(ts):
        ts = ts.strip()
        try:
            # 1. "ì˜¤ì „/ì˜¤í›„ HH:MM" ë˜ëŠ” "AM/PM HH:MM" ì²˜ë¦¬
            is_pm = 'ì˜¤í›„' in ts or 'PM' in ts or 'pm' in ts
            is_am = 'ì˜¤ì „' in ts or 'AM' in ts or 'am' in ts
            
            # ìˆ«ìì™€ ì½œë¡ ë§Œ ë‚¨ê¸°ê³  ì œê±°
            # ìˆ«ìì™€ ì½œë¡ ë§Œ ë‚¨ê¸°ê³  ì œê±° (ë‚ ì§œ ì œì™¸í•˜ê³  ì‹œê°„ë§Œ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ìˆ˜ì •)
            # Ex: "2024ë…„ 5ì›” 20ì¼ ì˜¤í›„ 3:15" -> "3:15" ì¶”ì¶œ
            time_match = re.search(r"(\d{1,2}):(\d{2})", ts)
            if not time_match: return -1
            hh, mm = int(time_match.group(1)), int(time_match.group(2))
            
            if is_pm and hh != 12: hh += 12
            elif is_am and hh == 12: hh = 0
            
            return hh * 60 + mm
        except:
            return -1

    m1 = time_to_min(t1_str)
    m2 = time_to_min(t2_str)
    
    if m1 == -1 or m2 == -1: return 0 # íŒŒì‹± ì‹¤íŒ¨ì‹œ 0 ì²˜ë¦¬
    
    diff = m2 - m1
    # ìì •ì„ ë„˜ê¸´ ê²½ìš° (ì˜ˆ: 23:50 -> 00:10 = -1420ë¶„ -> +20ë¶„?)
    # ë‹¨ìˆœí•˜ê²Œ í•˜ë£¨ 24ì‹œê°„ì„ ë”í•´ë´„ (ë‹¨, 12ì‹œê°„ ì´ìƒ ì°¨ì´ë‚˜ë©´ ë‚ ì§œ ë³€ê²½ìœ¼ë¡œ ê°„ì£¼)
    if diff < -720: diff += 1440 
    
    return max(0, diff) # ìŒìˆ˜ëŠ” 0 ì²˜ë¦¬

# -----------------------------------------------------------------------------
# [í•µì‹¬] MBTI Feature Extraction
# -----------------------------------------------------------------------------
def analyze_mbti_features(rows, target_name):
    # 1. Init Features
    feats = {
        'total_msgs': 0,
        'avg_reply_time': 0.0,
        'initiation_count': 0,
        'turn_length_avg': 0.0,
        'e_score': 0.0, 'i_score': 0.0,
        's_score': 0.0, 'n_score': 0.0,
        't_score': 0.0, 'f_score': 0.0,
        'j_score': 0.0, 'p_score': 0.0
    }
    
    target_sentences = []
    
    # 2. Interaction Analysis (Reply Speed, Initiation)
    reply_times = []
    consecutive_counts = []
    curr_consecutive = 0
    last_speaker = None
    last_time = None
    
    for r in rows:
        speaker = r['speaker']
        msg_time = r['time']
        text = r['text']
        
        # --- Target Logic
        if speaker == target_name:
            if text in SKIP_TOKENS: continue
            
            cleaned = clean_text(text)
            if cleaned: target_sentences.append(cleaned)
            
            feats['total_msgs'] += 1
            curr_consecutive += 1
            
            # ë‹µì¥ ì†ë„ (ì´ì „ í™”ìê°€ ë‹¤ë¥¸ ì‚¬ëŒì´ì—ˆì„ ë•Œ)
            if last_speaker and last_speaker != target_name and last_time:
                diff = parse_time_diff(last_time, msg_time)
                # 6ì‹œê°„(360ë¶„) ì´ìƒ ì§€ë‚¬ìœ¼ë©´ 'ì„ í†¡'ìœ¼ë¡œ ê°„ì£¼, ë‹µì¥ ì‹œê°„ ì œì™¸
                if diff > 360:
                    feats['initiation_count'] += 1
                else:
                    reply_times.append(diff)
                    
        else:
            # íƒ€ê²Ÿì´ ë§ì„ ëë‚´ê³  í„´ì´ ë„˜ì–´ê°
            if last_speaker == target_name:
                consecutive_counts.append(curr_consecutive)
                curr_consecutive = 0
                
        last_speaker = speaker
        last_time = msg_time

    # 3. Aggregation interaction
    if reply_times:
        feats['avg_reply_time'] = sum(reply_times) / len(reply_times)
    else:
        feats['avg_reply_time'] = 10.0 # Default
        
    if consecutive_counts:
        feats['turn_length_avg'] = sum(consecutive_counts) / len(consecutive_counts)
    else:
        feats['turn_length_avg'] = 1.0

    return feats, target_sentences

def analyze_linguistic_features(sentences, feats):
    # Kiwi Setup
    full_text = " ".join(sentences[:5000]) # Sample limit
    tokens = kiwi.tokenize(full_text)
    
    total_words = len(tokens)
    if total_words == 0: total_words = 1
    
    # Check simple keyword mapping first
    # Using raw morphemes for keyword matching
    morphs = [t.form for t in tokens]
    pos_tags = [t.tag for t in tokens] # Not heavily used yet, but good for filtering
    
    # --- Scoring Logic ---
    # [E vs I]
    # E: ì§ˆë¬¸/ê¶Œìœ  ì–´ë¯¸, ì§§ì€ í„´, ì„ í†¡ ë§ìŒ
    # I: ì„œìˆ í˜• ì–´ë¯¸, ê¸´ í„´, ë‹µì¥ ëŠë¦¼(Interactionì—ì„œ ì²˜ë¦¬)
    
    # Ending Analysis
    for t in tokens:
        m, tag = t.form, t.tag
        
        # S vs N
        # [ìˆ˜ì •] S ê°€ì¤‘ì¹˜ ë„ˆí”„, N ê°€ì¤‘ì¹˜ ë²„í”„
        if m in S_KEYWORDS: 
            feats['s_score'] += 1.0 # (ê¸°ì¡´ 1.5)
        elif tag == 'SN': # ìˆ«ì
            feats['s_score'] += 0.5 # (ê¸°ì¡´ 1.5 - ìˆ«ìëŠ” ë„ˆë¬´ ë§ì´ ë‚˜ì˜´)

        if m in N_KEYWORDS:
            feats['n_score'] += 2.0 # (ê¸°ì¡´ 1.5)
        # ê°€ì •/ì¶”ìƒ í‘œí˜„: 'ë“¯', 'ê²ƒ', 'ìˆ˜' (ì˜ì¡´ëª…ì‚¬ NNB) -> N ì„±í–¥ ì•½ê°„
        if tag == 'NNB': 
            feats['n_score'] += 0.5

        # T vs F
        # ë¶€ì‚¬(MAG, MAJ) ì ‘ì†ì‚¬(MAJ) ê°íƒ„ì‚¬(IC)
        if m in T_KEYWORDS:
            feats['t_score'] += 2.0
        if m in F_KEYWORDS:
            feats['f_score'] += 2.0
            
        # J vs P
        if m in J_KEYWORDS:
            feats['j_score'] += 2.0
        if m in P_KEYWORDS:
            feats['p_score'] += 2.0

        # E vs I (Endings)
        # ì¢…ê²°ì–´ë¯¸(EF)
        if tag.startswith('E'):
            if any(e in m for e in E_ENDINGS): feats['e_score'] += 1.0
            if any(e in m for e in I_ENDINGS): feats['i_score'] += 1.0
            
    # Normalize by text length (per 100 words)
    scale = 100 / total_words
    feats['s_score'] *= scale
    feats['n_score'] *= scale
    feats['t_score'] *= scale
    feats['f_score'] *= scale
    feats['j_score'] *= scale
    feats['p_score'] *= scale
    feats['e_score'] *= scale
    feats['i_score'] *= scale

    # [ìˆ˜ì •] í‚¤ì›Œë“œ ì ìˆ˜ ë¹„ì¤‘ ì¶•ì†Œ (í–‰ë™ ì ìˆ˜ ê°•ì¡°ë¥¼ ìœ„í•´ 0.2ë°° ì ìš©)
    # [ìˆ˜ì •] E/IëŠ” í–‰ë™ ì ìˆ˜ê°€ í¬ë¯€ë¡œ í‚¤ì›Œë“œ ë¹„ì¤‘ì„ ë‚®ì¶¤ (0.2ë°°)
    for k in ['e_score', 'i_score']:
        feats[k] *= 0.2

    # [ìˆ˜ì •] ë‚˜ë¨¸ì§€ëŠ” í–‰ë™ ì ìˆ˜ê°€ ì—†ìœ¼ë¯€ë¡œ, í‚¤ì›Œë“œ ì ìˆ˜ë¥¼ ëŒ€í­ ìƒí–¥ (5.0ë°°)
    # ì •ì œëœ í¬ê·€ í‚¤ì›Œë“œì´ë¯€ë¡œ ë°œê²¬ ì‹œ ë†’ì€ ì ìˆ˜ ë¶€ì—¬ í•„ìš”
    for k in ['s_score', 'n_score', 't_score', 'f_score', 'j_score', 'p_score']:
        # [ìˆ˜ì •] í”í•œ ë‹¨ì–´ ë³µêµ¬ë¡œ ë¹ˆë„ê°€ ëŠ˜ì—ˆìœ¼ë¯€ë¡œ ê°€ì¤‘ì¹˜ ì†Œí­ í•˜í–¥ (5.0 -> 3.0)
        feats[k] *= 3.0
    
    return feats

def calculate_final_mbti(feats):
    # Rule-Set Weights
    
    # 1. Extraversion (E) vs Introversion (I)
    # E factors: Fast Reply (< 2 min), Initiation, Turn Length(Short & Frequent), Endings
    # I factors: Slow Reply (> 5 min), Long Turn, Endings
    
    # [ìˆ˜ì •] ë‹µì¥ ì†ë„ ê¸°ì¤€ ì™„í™” ë° I ì ìˆ˜ ê°•í™”
    # í•œêµ­ì¸ íŠ¹ì„±ìƒ 'ë¹¨ë¦¬ë¹¨ë¦¬'ê°€ ë§ì•„ Eê°€ ê³¼ëŒ€í‰ê°€ë¨. ê¸°ì¤€ì„ ë” ì—„ê²©í•˜ê²Œ.
    # [ìˆ˜ì •] ë‹µì¥ ì†ë„ (ë¹„ë¡€ ì ìˆ˜ì œ): 3ë¶„ ì´ë‚´ë©´ ì ìˆ˜ ë¶€ì—¬ (ë¹ ë¥¼ìˆ˜ë¡ ê³ ë“ì , ìµœëŒ€ 1.0ì /ê¸°ë³¸ ê°€ì¤‘ì¹˜ 1.0)
    # 0ë¶„(ì¦‰ì‹œ) -> 1.0ì , 1.5ë¶„ -> 0.5ì , 3ë¶„ -> 0ì 
    reply_score = max(0, (3.0 - feats['avg_reply_time']) / 3.0)
    # [ìˆ˜ì •] E ê³¼ëŒ€í‰ê°€ ë°©ì§€: ë‹µì¥ ì†ë„ ê°€ì¤‘ì¹˜ 1.0 -> 0.5ë¡œ ì¶•ì†Œ
    feats['e_score'] += min(0.5, reply_score * 0.5)
    feats['e_score'] += min(1.0, reply_score)
    
    # ëŠë¦° ë‹µì¥(I ì„±í–¥)ì€ ê¸°ì¡´ ìœ ì§€ (5ë¶„ ì´ìƒì´ë©´ I +3.0)
    # [ìˆ˜ì •] ëŠë¦° ë‹µì¥ ê¸°ì¤€ ì™„í™”: 3ë¶„ë§Œ ë„˜ì–´ë„ I ì ìˆ˜ ë¶€ì—¬
    if feats['avg_reply_time'] > 3.0: feats['i_score'] += 3.0
    
    # [ìˆ˜ì •] í„´ ê¸¸ì´ (ë¹„ë¡€ ì ìˆ˜ì œ): 2.5ì–´ì ˆ ë¯¸ë§Œì´ë©´ ì ìˆ˜ ë¶€ì—¬ (ì§§ì„ìˆ˜ë¡ ê³ ë“ì , ìµœëŒ€ 1.0ì )
    turn_score = max(0, (2.5 - feats['turn_length_avg']) * 0.6)
    feats['e_score'] += min(1.0, turn_score)
    
    # ê¸´ í„´(I ì„±í–¥)ì€ ê¸°ì¡´ ìœ ì§€
    if feats['turn_length_avg'] >= 2.0: feats['i_score'] += 2.0
    
    # [ìˆ˜ì •] Initiation (ë¹„ë¡€ ì ìˆ˜ì œ): íšŸìˆ˜ë‹¹ 0.8ì  (ìµœëŒ€ 4.0ì )
    # 1íšŒ: 0.8ì , 3íšŒ: 2.4ì , 5íšŒ+: 4.0ì 
    # [ìˆ˜ì •] ì„ í†¡ 3íšŒë¶€í„° ì¸ì • (1~2íšŒëŠ” ë¬´ì‹œ)
    # [ìˆ˜ì •] ì„ í†¡ ì ìˆ˜ ì™„í™”: 0.5ì ì”© ì²œì²œíˆ ì¦ê°€ (ìµœëŒ€ 3.0ì )
    # [ìˆ˜ì •] ì„ í†¡ì´ ê°€ì¥ ì¤‘ìš”í•¨: ê°€ì¤‘ì¹˜ ëŒ€í­ ìƒí–¥ (ìµœëŒ€ 15.0ì )
    # 1~2íšŒ ë¬´ì‹œ, 3íšŒë¶€í„° íšŒë‹¹ 1.5ì ì”© íŒíŒ ë¶€ì—¬
    init_score = max(0, (feats['initiation_count'] - 2) * 1.5)
    feats['e_score'] += min(15.0, init_score)
    # [ìˆ˜ì •] I ë³´ë„ˆìŠ¤: ì„ í†¡ì´ ì ìœ¼ë©´(2íšŒ ì´í•˜) ë‚´í–¥í˜• ì ìˆ˜ ë¶€ì—¬ (+5.0)
    if feats['initiation_count'] <= 2: feats['i_score'] += 5.0
    # E/I Decision
    e_total = feats['e_score'] * WEIGHT_E
    i_total = feats['i_score'] * WEIGHT_I
    
    # [ìˆ˜ì •] ì‚¬ìš©ìê°€ ì„¤ì •í•œ ê°€ì¤‘ì¹˜ ì ìš©
    e_final = 'E' if e_total > i_total else 'I'
    
    # 2. Sensing (S) vs Intuition (N)
    # [ìˆ˜ì •] ì‚¬ìš©ìê°€ ì„¤ì •í•œ ê°€ì¤‘ì¹˜ ì ìš©
    s_score_final = feats['s_score'] * WEIGHT_S
    n_score_final = feats['n_score'] * WEIGHT_N
    s_final = 'S' if s_score_final > n_score_final else 'N'
    
    # 3. Thinking (T) vs Feeling (F)
    t_score_final = feats['t_score'] * WEIGHT_T
    f_score_final = feats['f_score'] * WEIGHT_F
    f_final = 'F' if f_score_final > t_score_final else 'T'
    
    # 4. Judging (J) vs Perceiving (P)
    j_score_final = feats['j_score'] * WEIGHT_J
    p_score_final = feats['p_score'] * WEIGHT_P
    j_final = 'J' if j_score_final > p_score_final else 'P'
    
    mbti = f"{e_final}{s_final}{f_final}{j_final}"
    
    # Reasoning Generation
    reasons = []
    
    # E/I Reason
    if e_final == 'E': reasons.append(f"í‰ê·  ë‹µì¥ ì†ë„ê°€ {feats['avg_reply_time']:.1f}ë¶„ìœ¼ë¡œ ë¹ ë¥´ê³ , ëŒ€í™”ë¥¼ ìì£¼ ì£¼ë„í•˜ì—¬ **ì™¸í–¥í˜•(E)** íŠ¹ì„±ì„ ë³´ì…ë‹ˆë‹¤.")
    else: reasons.append(f"í‰ê·  ë‹µì¥ ì‹œê°„ì´ {feats['avg_reply_time']:.1f}ë¶„ìœ¼ë¡œ ì‹ ì¤‘í•˜ë©°, í•œë²ˆì— ê¸´ ë‚´ìš©ì„ ë‹´ì•„ **ë‚´í–¥í˜•(I)** ì„±í–¥ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")
    
    # S/N Reason
    if s_final == 'S': reasons.append(f"êµ¬ì²´ì ì¸ ìˆ«ìì™€ í˜„ì‹¤ì ì¸ ë‹¨ì–´ì˜ ë¹„ì¤‘({feats['s_score']:.1f})ì´ ì¶”ìƒì  í‘œí˜„ë³´ë‹¤ ë†’ì•„ **ê°ê°í˜•(S)**ì…ë‹ˆë‹¤.")
    else: reasons.append(f"ê°€ì •ë²•ì´ë‚˜ ì¶”ìƒì ì¸ ì–´íœ˜ì˜ ë¹„ì¤‘({feats['n_score']:.1f})ì´ ë†’ì•„ ìƒìƒë ¥ì´ í’ë¶€í•œ **ì§ê´€í˜•(N)**ì…ë‹ˆë‹¤.")
    
    # T/F Reason
    if f_final == 'F': reasons.append(f"ê³µê°ê³¼ ë¦¬ì•¡ì…˜ ë‹¨ì–´ì˜ ë¹ˆë„({feats['f_score']:.1f})ê°€ ë…¼ë¦¬ì  í‘œí˜„ë³´ë‹¤ ì›”ë“±íˆ ë†’ì•„ **ê°ì •í˜•(F)**ì…ë‹ˆë‹¤.")
    else: reasons.append(f"ì›ì¸ê³¼ ê²°ê³¼ë¥¼ ë”°ì§€ëŠ” ë…¼ë¦¬ì  ë‹¨ì–´({feats['t_score']:.1f})ë¥¼ ìì£¼ ì‚¬ìš©í•˜ì—¬ **ì‚¬ê³ í˜•(T)**ì…ë‹ˆë‹¤.")
    
    # J/P Reason
    if j_final == 'J': reasons.append(f"ê³„íš ë° ì¼ì •ì„ ì–¸ê¸‰í•˜ëŠ” ë¹ˆë„({feats['j_score']:.1f})ê°€ ë†’ì•„ ê³„íšì ì¸ **íŒë‹¨í˜•(J)**ì…ë‹ˆë‹¤.")
    else: reasons.append(f"ìƒí™©ì— ë”°ë¥¸ ë³€ë™ì„±ì´ë‚˜ ìœ ì—°í•œ í‘œí˜„({feats['p_score']:.1f})ì´ ë§ì•„ ì¦‰í¥ì ì¸ **ì¸ì‹í˜•(P)**ì…ë‹ˆë‹¤.")

    # [ì¶”ê°€] ìƒì„¸ ìˆ˜ì¹˜ í‘œê¸°
    reasons.append(f"""
    <div style='background:#f8f9fa; padding:10px; border-radius:5px; margin-top:10px; font-size:0.9em;'>
        <strong>ğŸ“Š ìƒì„¸ ì§€í‘œ ì ìˆ˜ (ê°€ì¤‘ì¹˜ ì ìš©ë¨)</strong><br>
        E({e_total:.1f}) vs I({i_total:.1f})<br>
        S({s_score_final:.1f}) vs N({n_score_final:.1f})<br>
        T({t_score_final:.1f}) vs F({f_score_final:.1f})<br>
        J({j_score_final:.1f}) vs P({p_score_final:.1f})
    </div>
    """)

    return mbti, "<br>".join(reasons), feats

# -----------------------------------------------------------------------------
# [ì¶”ê°€] ê°ì„±/ë…ì„± ê¸°ë°˜ Big5 ì •ë°€ ì‚°ì¶œ
# -----------------------------------------------------------------------------
def analyze_sentiment_score(sentences):
    """
    KNU ê°ì„± ì‚¬ì „ì„ ì´ìš©í•˜ì—¬ ë¬¸ì¥ë“¤ì˜ í‰ê·  ê¸ì •/ë¶€ì • ì ìˆ˜ë¥¼ ê³„ì‚°
    Return: (pos_ratio, neg_ratio, avg_sentiment)
    """
    total_score = 0
    pos_score = 0
    neg_score = 0
    word_count = 0
    
    # KNU ì‚¬ì „ì„ í™œìš© (SENTIMENT_DB)
    for sent in sentences:
        # ê°„ë‹¨í•œ í† í°í™” (ë„ì–´ì“°ê¸° ê¸°ì¤€) ë˜ëŠ” Kiwi í† í° í™œìš© ê°€ëŠ¥
        # ì—¬ê¸°ì„œëŠ” Kiwi í† í°í™”ê°€ ì´ë¯¸ ë˜ì–´ìˆì§€ ì•Šìœ¼ë¯€ë¡œ ê°„ë‹¨íˆ ì²˜ë¦¬í•˜ê±°ë‚˜,
        # app.py ìƒë‹¨ì˜ kiwipiepyëŠ” analyze_linguistic_featuresì—ì„œë§Œ ì“°ì„.
        # ì„±ëŠ¥ì„ ìœ„í•´ ë„ì–´ì“°ê¸° + ì¼ë¶€ ì¡°ì‚¬ ì œê±° ë§¤ì¹­ ì‹œë„
        words = sent.split()
        for w in words:
            # ì¡°ì‚¬ ì œê±° ë“±ì˜ ì •ê·œí™”ëŠ” ë³µì¡í•˜ë¯€ë¡œ, ìˆëŠ” ê·¸ëŒ€ë¡œ ë§¤ì¹­í•˜ë˜
            # ì‚¬ì „ì— ìˆëŠ” ë£¨íŠ¸ ë‹¨ì–´ë„ ë§¤ì¹­ (ì •í™•ë„ëŠ” ë–¨ì–´ì§ˆ ìˆ˜ ìˆìœ¼ë‚˜ ì†ë„ ì¤‘ìš”)
            s = SENTIMENT_DB.get(w)
            if s is not None:
                word_count += 1
                total_score += s
                if s > 0: pos_score += s
                elif s < 0: neg_score += abs(s)
    
    if word_count == 0: return 0.0, 0.0, 0.0
    
    # 0~100ì  ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™”ëœ ì§€í‘œë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ë¹„ìœ¨ ê³„ì‚°
    # (ë‹¨ìˆœ ê°œìˆ˜ê°€ ì•„ë‹ˆë¼ ì ìˆ˜ ë¹„ì¤‘)
    total_abs = pos_score + neg_score
    if total_abs == 0: return 0.0, 0.0, 0.0
    
    pos_ratio = pos_score / total_abs # 0.0 ~ 1.0
    neg_ratio = neg_score / total_abs # 0.0 ~ 1.0
    
    return pos_ratio, neg_ratio, total_score




# -----------------------
# [3] Flask ë¼ìš°íŒ…
# -----------------------
def get_db_connection():
    return pymysql.connect(**db_config)

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

@app.route('/')
def index():
    if 'user_id' in session: return redirect(url_for('upload_page'))
    return redirect(url_for('login'))

@app.route('/register', methods=['GET'])
def register_page(): return render_template('register.html')

@app.route('/api/register', methods=['POST'])
def register_api():
    try:
        email = request.form['email']
        password = request.form['password']
        username = request.form['username']
        nickname = request.form['nickname']
        gender = request.form['gender']
        birth_date = request.form['birth_date']
        hashed_password = generate_password_hash(password)

        conn = get_db_connection()
        with conn.cursor() as cursor:
            sql = "INSERT INTO users (email, password_hash, username, nickname, gender, birth_date) VALUES (%s, %s, %s, %s, %s, %s)"
            cursor.execute(sql, (email, hashed_password, username, nickname, gender, birth_date))
        conn.commit()
        conn.close()
        flash("íšŒì›ê°€ì… ì™„ë£Œ. ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
        return redirect(url_for('login'))
    except Exception as e:
        flash(f"íšŒì›ê°€ì… ì‹¤íŒ¨: {e}")
        return redirect(url_for('register_page'))

@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        email = request.form['email']
        password = request.form['password']
        conn = get_db_connection()
        try:
            with conn.cursor() as cursor:
                sql = "SELECT * FROM users WHERE email = %s"
                cursor.execute(sql, (email,))
                user = cursor.fetchone()
                if user and check_password_hash(user['password_hash'], password):
                    session['user_id'] = user['user_id']
                    session['nickname'] = user['nickname']
                    return redirect(url_for('upload_page'))
                else:
                    flash("ì´ë©”ì¼ ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
        finally:
            conn.close()
    return render_template('login.html')

@app.route('/logout')
def logout():
    session.clear()
    return redirect(url_for('login'))

# [ì¶”ê°€] ê²ŒìŠ¤íŠ¸ ë¡œê·¸ì¸ ë¡œì§
def get_or_create_guest_user():
    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            # 1. Check if guest exists
            sql = "SELECT * FROM users WHERE email = 'guest@echomind.com'"
            cursor.execute(sql)
            user = cursor.fetchone()
            
            if user:
                return user
            
            # 2. Create if not exists
            # ë¹„ë°€ë²ˆí˜¸ëŠ” ëœë¤ í˜¹ì€ ê³ ì •ê°’ (ë¡œê·¸ì¸í•  ì¼ì´ ì—†ìœ¼ë¯€ë¡œ í¬ê²Œ ì¤‘ìš”ì¹˜ ì•ŠìŒ)
            guest_pw = generate_password_hash("guest1234") 
            sql_insert = """
                INSERT INTO users (email, password_hash, username, nickname, gender, birth_date)
                VALUES ('guest@echomind.com', %s, 'GuestUser', 'ê²ŒìŠ¤íŠ¸', 'Non-Binary', '2000-01-01')
            """
            cursor.execute(sql_insert, (guest_pw,))
            conn.commit()
            
            # 3. Retrieve created user
            cursor.execute(sql)
            return cursor.fetchone()
    finally:
        conn.close()

@app.route('/guest_login')
def guest_login():
    try:
        user = get_or_create_guest_user()
        if user:
            session['user_id'] = user['user_id']
            session['nickname'] = user['nickname']
            flash("ë¹„íšŒì›(ê²ŒìŠ¤íŠ¸)ë¡œ ë¡œê·¸ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return redirect(url_for('upload_page'))
        else:
            flash("ê²ŒìŠ¤íŠ¸ ë¡œê·¸ì¸ ì‹¤íŒ¨: ê³„ì • ìƒì„± ì˜¤ë¥˜")
            return redirect(url_for('login'))
    except Exception as e:
        flash(f"Error: {e}")
        return redirect(url_for('login'))

@app.route('/upload', methods=['GET'])
def upload_page():
    if 'user_id' not in session: return redirect(url_for('login'))
    return render_template('upload.html', nickname=session['nickname'])

@app.route('/api/upload_chat', methods=['POST'])
def upload_api():
    print(">>> [DEBUG] /api/upload_chat í˜¸ì¶œë¨")
    sys.stdout.flush()

    if 'user_id' not in session: 
        print(">>> [DEBUG] ë¡œê·¸ì¸ ì„¸ì…˜ ì—†ìŒ -> redirect")
        sys.stdout.flush()
        return redirect(url_for('login'))

    if 'chat_file' not in request.files: 
        print(">>> [DEBUG] íŒŒì¼ ì—†ìŒ -> redirect")
        sys.stdout.flush()
        return redirect(request.url)
    
    file = request.files['chat_file']
    target_name = request.form.get('target_name', '').strip()
    
    print(f">>> [DEBUG] íŒŒì¼ëª…: {file.filename}, ë¶„ì„ëŒ€ìƒ: {target_name}")
    sys.stdout.flush()

    if not target_name:
        flash('ë¶„ì„í•  ëŒ€í™”ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.')
        return redirect(request.url)

    if file and allowed_file(file.filename):
        filename = secure_filename(file.filename)
        save_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{filename}"
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], save_name)
        file.save(file_path)
        
        user_id = session['user_id']
        conn = get_db_connection()
        
        try:
            rows = parse_kakao_txt(file_path)
            my_sentences = []
            for r in rows:
                if r["speaker"] == target_name:
                    txt = r["text"]
                    if txt and txt not in SKIP_TOKENS:
                        cleaned = clean_text(txt)
                        if cleaned: my_sentences.append(cleaned)
            
            if len(my_sentences) < 5:
                flash(f"'{target_name}'ë‹˜ì˜ ëŒ€í™” ë‚´ìš©ì´ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ì´ë¦„ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return redirect(url_for('upload_page'))

            # [í•µì‹¬] ì‹ ê·œ ì–¸ì–´ ë¶„ì„ í•¨ìˆ˜ í˜¸ì¶œ
            # 1. ìƒí˜¸ì‘ìš© ë° ê¸°ë³¸ íŠ¹ì§• ì¶”ì¶œ
            full_features, target_sentences = analyze_mbti_features(rows, target_name)
            
            if len(target_sentences) < 5:
                flash(f"'{target_name}'ë‹˜ì˜ ëŒ€í™” ë‚´ìš©ì´ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤(5ë¬¸ì¥ ë¯¸ë§Œ).")
                return redirect(url_for('upload_page'))

            # 2. ì–¸ì–´ì  íŠ¹ì§• ì‹¬í™” ë¶„ì„ (Kiwi)
            full_features = analyze_linguistic_features(target_sentences, full_features)
            
            # 3. MBTI ë° ì„¤ëª… ìƒì„±
            # [ìˆ˜ì •] í•¨ìˆ˜ë¥¼ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•˜ë©´ feats ì ìˆ˜ê°€ ì¤‘ë³µ ëˆ„ì ë˜ëŠ” ë²„ê·¸ ìˆ˜ì •
            # í•œë²ˆë§Œ í˜¸ì¶œí•´ì„œ ê²°ê³¼ë¥¼ unpacking
            mbti_prediction, reasoning_text, debug_feats = calculate_final_mbti(full_features)

            # 4. ì •ë°€ í†µê³„ (ë…ì„±, ê¸ë¶€ì •, ìŠ¤íƒ€ì¼)
            tox_count = 0
            for s in target_sentences:
                # ë…ì„± (korean_bad_words.json + ê¸°ë³¸ ìš•ì„¤)
                if any(bad in s for bad in BAD_WORDS): tox_count += 1
            
            total_sent = len(target_sentences)
            tox_ratio = tox_count / total_sent if total_sent else 0.0
            
            # KNU ê°ì„± ë¶„ì„
            pos_ratio, neg_ratio, _ = analyze_sentiment_score(target_sentences)
            
            # [ì‹ ê·œ] í•œêµ­ì–´ ìŠ¤íƒ€ì¼ ì‹¬ì¸µ ë¶„ì„ (TTR, ë¦¬ì•¡ì…˜ ë“±)
            style_feats = analyze_korean_style_features(target_sentences)
            
            # 5. Big5 ì‚°ì¶œ (ìŠ¤íƒ€ì¼ + ê°ì„± + ë…ì„± ê¸°ë°˜ ê³ ë„í™”)
            big5_result = calculate_advanced_big5(style_feats, tox_ratio, pos_ratio, neg_ratio)

            # [ë””ë²„ê¹…] ì¤‘ê°„ ê³„ì‚° ê°’ì„ ì½˜ì†”ì— ì¶œë ¥
            print("\n" + "="*50)
            print(f"   [ìƒì„¸ ì„±í–¥ ë¶„ì„ - ëŒ€ìƒ: {target_name}]")
            print("="*50)
            print(f"1. MBTI ê²°ì •: {mbti_prediction}")
            print(f"   - E: {debug_feats['e_score']:.1f} vs I: {debug_feats['i_score']:.1f}")
            print(f"   - S: {debug_feats['s_score']:.1f} vs N: {debug_feats['n_score']:.1f}")
            print(f"   - T: {debug_feats['t_score']:.1f} vs F: {debug_feats['f_score']:.1f}")
            print(f"   - J: {debug_feats['j_score']:.1f} vs P: {debug_feats['p_score']:.1f}")
            print(f"2. ì–¸ì–´ íŠ¹ì§•: ë…ì„± {tox_ratio*100:.1f}%, ê¸ì • {pos_ratio*100:.1f}%, TTR {style_feats['ttr']:.2f}, ë¦¬ì•¡ì…˜ {style_feats['laughs']:.2f}")
            print(f"3. Big5 ì¶”ë¡ : Open({big5_result['openness']}), Consc({big5_result['conscientiousness']}), Extra({big5_result['extraversion']}), Agree({big5_result['agreeableness']}), Neuro({big5_result['neuroticism']})")
            print("="*50 + "\n")
            sys.stdout.flush()  # [ì¶”ê°€] ì¶œë ¥ ê°•ì œ í”ŒëŸ¬ì‹œ
            
            summary_text = f"ë¶„ì„ ë¬¸ì¥: {total_sent}ê°œ. ì •ë°€ ìŠ¤íƒ€ì¼ ë¶„ì„ ê¸°ë°˜ ì„±í–¥ ë„ì¶œ."

            with conn.cursor() as cursor:
                sql_log = "INSERT INTO chat_logs (user_id, file_name, file_path, target_name, process_status) VALUES (%s, %s, %s, %s, 'COMPLETED')"
                cursor.execute(sql_log, (user_id, filename, file_path, target_name))
                log_id = cursor.lastrowid
                
                sql_result = """
                INSERT INTO personality_results 
                (user_id, log_id, openness, conscientiousness, extraversion, agreeableness, neuroticism, 
                 summary_text, mbti_prediction, reasoning_text, toxicity_score, sentiment_pos_ratio, sentiment_neg_ratio)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                """
                cursor.execute(sql_result, (
                    user_id, log_id,
                    big5_result['openness'], big5_result['conscientiousness'], 
                    big5_result['extraversion'], big5_result['agreeableness'], 
                    big5_result['neuroticism'], 
                    summary_text, mbti_prediction, reasoning_text,
                    tox_ratio, pos_ratio, neg_ratio
                ))
            conn.commit()
            return redirect(url_for('result_page', log_id=log_id))
            
        except Exception as e:
            conn.rollback()
            print(f"Error: {e}")
            flash(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return redirect(url_for('upload_page'))
        finally:
            conn.close()
    return redirect(url_for('upload_page'))

@app.route('/result/<int:log_id>')
def result_page(log_id):
    if 'user_id' not in session: return redirect(url_for('login'))
    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            sql = """
            SELECT r.*, l.file_name, l.target_name 
            FROM personality_results r
            JOIN chat_logs l ON r.log_id = l.log_id
            WHERE r.log_id = %s AND r.user_id = %s
            """
            cursor.execute(sql, (log_id, session['user_id']))
            result = cursor.fetchone()
            if not result:
                flash("ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return redirect(url_for('upload_page'))
            return render_template('result.html', data=result, nickname=session['nickname'])
    finally:
        conn.close()

if __name__ == '__main__':
    app.run(host='0.0.0.0', debug=True, port=5000)
